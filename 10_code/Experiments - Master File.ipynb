{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Compression and Accleration Experiments on BERT and TinyLlama-1.1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1) If you have time, create better graphs, or tables for your outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the aggregate experimentation results for our network compression and acceleration methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"..\")\n",
    "importlib.reload(sys.modules['CompressionMethods.distillation'])\n",
    "#importlib.reload(sys.modules['CompressionMethods.utils'])\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from CompressionMethods.BERTFineTuning import BERTFineTuning\n",
    "from CompressionMethods.distillation import DistillationModule\n",
    "from CompressionMethods.GPTQQuantizer import GPTQQuantizer\n",
    "from CompressionMethods.utils import utils\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning BERT on Multilabel Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 886/886 [00:00<00:00, 10877.59 examples/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4275' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4275/4275 06:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.319140</td>\n",
       "      <td>0.675318</td>\n",
       "      <td>0.772590</td>\n",
       "      <td>0.288939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.309701</td>\n",
       "      <td>0.704274</td>\n",
       "      <td>0.798457</td>\n",
       "      <td>0.287810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.310101</td>\n",
       "      <td>0.706287</td>\n",
       "      <td>0.801536</td>\n",
       "      <td>0.276524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.310814</td>\n",
       "      <td>0.708271</td>\n",
       "      <td>0.801399</td>\n",
       "      <td>0.274266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.314582</td>\n",
       "      <td>0.709167</td>\n",
       "      <td>0.802292</td>\n",
       "      <td>0.276524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 403.6719915866852\n"
     ]
    }
   ],
   "source": [
    "bft = BERTFineTuning(\"bert-base-uncased\")\n",
    "bft.get_device()\n",
    "finetuned_bert = bft.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 417.682MB\n",
      "count    886.000000\n",
      "mean       0.013172\n",
      "std        0.018991\n",
      "min        0.012010\n",
      "25%        0.012294\n",
      "50%        0.012448\n",
      "75%        0.012603\n",
      "max        0.577584\n",
      "dtype: float64\n",
      "{'f1': 0.6886717718510694, 'roc_auc': 0.7804860661385984, 'accuracy': 0.2742663656884876}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "utils_bert_finetuned = utils('./bert-finetuned', dataset = dataset)\n",
    "utils_bert_finetuned.get_model_size()\n",
    "utils_bert_finetuned.evaluate_bert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4275' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4275/4275 07:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.238000</td>\n",
       "      <td>0.427604</td>\n",
       "      <td>0.688204</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.264108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>0.332423</td>\n",
       "      <td>0.694124</td>\n",
       "      <td>0.790821</td>\n",
       "      <td>0.282167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.305789</td>\n",
       "      <td>0.697990</td>\n",
       "      <td>0.793034</td>\n",
       "      <td>0.278781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.294430</td>\n",
       "      <td>0.697372</td>\n",
       "      <td>0.793234</td>\n",
       "      <td>0.275395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.292132</td>\n",
       "      <td>0.697326</td>\n",
       "      <td>0.793433</td>\n",
       "      <td>0.279910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  445.8245093822479\n"
     ]
    }
   ],
   "source": [
    "dm = DistillationModule()\n",
    "distilled_model = dm.perform_distillation(teacher_model_id = f'./bert-finetuned', student_model_id = 'distilbert/distilbert-base-uncased', dataset = dataset, num_labels = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 255.443MB\n",
      "count    886.000000\n",
      "mean       0.006462\n",
      "std        0.000528\n",
      "min        0.006187\n",
      "25%        0.006307\n",
      "50%        0.006387\n",
      "75%        0.006469\n",
      "max        0.014305\n",
      "dtype: float64\n",
      "{'f1': 0.6690159934941718, 'roc_auc': 0.766135608865375, 'accuracy': 0.2618510158013544}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "utils_bert_dist = utils('./bert-distilled', dataset = dataset)\n",
    "utils_bert_dist.get_model_size()\n",
    "utils_bert_dist.evaluate_bert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TinyLlama-1.1B GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at heegyu/TinyLlama-augesc-context and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Quantizing model.layers blocks : 100%|██████████| 22/22 [09:30<00:00, 25.91s/it]\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    }
   ],
   "source": [
    "llm = \"heegyu/TinyLlama-augesc-context\"\n",
    "gptq_quantizer = GPTQQuantizer(llm)\n",
    "dataset = load_dataset(\"heegyu/augesc\")\n",
    "x, y, label_map = utils(llm).process_tinyllama_dataset(dataset)\n",
    "llm_model_gptq = gptq_quantizer.quantize(x, \"heegyu/TinyLlama-augesc-context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model size:\n",
      "Model size: 3968.417MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./gptq-quantized-model and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 618.512MB\n"
     ]
    }
   ],
   "source": [
    "print(\"base model size:\")\n",
    "utils(llm).get_model_size()\n",
    "gptq_quantized_model = gptq_quantizer.load_model() #you need to run this for all GPTQ models for it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./gptq-quantized-model and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 618.512MB\n",
      "0.032\n",
      "count    1000.000000\n",
      "mean        0.058827\n",
      "std         0.020093\n",
      "min         0.054847\n",
      "25%         0.056873\n",
      "50%         0.057546\n",
      "75%         0.058401\n",
      "max         0.680813\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"heegyu/augesc\")\n",
    "utils_llm_gptq = utils('./gptq-quantized-model', dataset = dataset)\n",
    "utils_llm_gptq.get_model_size()\n",
    "utils_llm_gptq.evaluate_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
